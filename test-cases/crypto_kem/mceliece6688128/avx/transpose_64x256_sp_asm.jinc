//# FIX
//# FIX
u256 MASK0_0 = (4u64)[0x5555555555555555, 0x5555555555555555, 0x5555555555555555, 0x5555555555555555];
u256 MASK0_1 = (4u64)[0xAAAAAAAAAAAAAAAA, 0xAAAAAAAAAAAAAAAA, 0xAAAAAAAAAAAAAAAA, 0xAAAAAAAAAAAAAAAA];
u256 MASK1_0 = (4u64)[0x3333333333333333, 0x3333333333333333, 0x3333333333333333, 0x3333333333333333];
u256 MASK1_1 = (4u64)[0xCCCCCCCCCCCCCCCC, 0xCCCCCCCCCCCCCCCC, 0xCCCCCCCCCCCCCCCC, 0xCCCCCCCCCCCCCCCC];
u256 MASK2_0 = (4u64)[0x0F0F0F0F0F0F0F0F, 0x0F0F0F0F0F0F0F0F, 0x0F0F0F0F0F0F0F0F, 0x0F0F0F0F0F0F0F0F];
u256 MASK2_1 = (4u64)[0xF0F0F0F0F0F0F0F0, 0xF0F0F0F0F0F0F0F0, 0xF0F0F0F0F0F0F0F0, 0xF0F0F0F0F0F0F0F0];
u256 MASK3_0 = (4u64)[0x00FF00FF00FF00FF, 0x00FF00FF00FF00FF, 0x00FF00FF00FF00FF, 0x00FF00FF00FF00FF];
u256 MASK3_1 = (4u64)[0xFF00FF00FF00FF00, 0xFF00FF00FF00FF00, 0xFF00FF00FF00FF00, 0xFF00FF00FF00FF00];
u256 MASK4_0 = (4u64)[0x0000FFFF0000FFFF, 0x0000FFFF0000FFFF, 0x0000FFFF0000FFFF, 0x0000FFFF0000FFFF];
u256 MASK4_1 = (4u64)[0xFFFF0000FFFF0000, 0xFFFF0000FFFF0000, 0xFFFF0000FFFF0000, 0xFFFF0000FFFF0000];
u256 MASK5_0 = (4u64)[0x00000000FFFFFFFF, 0x00000000FFFFFFFF, 0x00000000FFFFFFFF, 0x00000000FFFFFFFF];
u256 MASK5_1 = (4u64)[0xFFFFFFFF00000000, 0xFFFFFFFF00000000, 0xFFFFFFFF00000000, 0xFFFFFFFF00000000];
export fn transpose_64x256_sp_asm(reg u64 input_0){
	 reg u256 mask0;
	 reg u256 mask1;
	 reg u256 mask2;
	 reg u256 mask3;
	 reg u256 mask4;
	 reg u256 mask5;
	 reg u256 v00;
	 reg u256 v01;
	 reg u256 v10;
	 reg u256 v11;
	 reg u256 x0;
	 reg u256 x1;
	 reg u256 x2;
	 reg u256 x3;
	 reg u256 x4;
	 reg u256 x5;
	 reg u256 x6;
	 reg u256 x7;

	mask0 = MASK5_0;
	mask1 = MASK5_1;
	mask2 = MASK4_0;
	mask3 = MASK4_1;
	mask4 = MASK3_0;
	mask5 = MASK3_1;
	x0 = (u256)[input_0 + 0];
	x1 = (u256)[input_0 + 256];
	x2 = (u256)[input_0 + 512];
	x3 = (u256)[input_0 + 768];
	x4 = (u256)[input_0 + 1024];
	x5 = (u256)[input_0 + 1280];
	x6 = (u256)[input_0 + 1536];
	x7 = (u256)[input_0 + 1792];
	v00 = x0 & mask0;
	v10 = x4 <<4u64 32;
	v01 = x0 >>4u64 32;
	v11 = x4 & mask1;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 <<4u64 32;
	v01 = x1 >>4u64 32;
	v11 = x5 & mask1;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 <<4u64 32;
	v01 = x2 >>4u64 32;
	v11 = x6 & mask1;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 <<4u64 32;
	v01 = x3 >>4u64 32;
	v11 = x7 & mask1;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 <<8u32 16;
	v01 = x0 >>8u32 16;
	v11 = x2 & mask3;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 <<8u32 16;
	v01 = x1 >>8u32 16;
	v11 = x3 & mask3;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 <<8u32 16;
	v01 = x4 >>8u32 16;
	v11 = x6 & mask3;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 <<8u32 16;
	v01 = x5 >>8u32 16;
	v11 = x7 & mask3;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 <<16u16 8;
	v01 = x0 >>16u16 8;
	v11 = x1 & mask5;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 <<16u16 8;
	v01 = x2 >>16u16 8;
	v11 = x3 & mask5;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 <<16u16 8;
	v01 = x4 >>16u16 8;
	v11 = x5 & mask5;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 <<16u16 8;
	v01 = x6 >>16u16 8;
	v11 = x7 & mask5;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 0] = x0;
	(u256)[input_0 + 256] = x1;
	(u256)[input_0 + 512] = x2;
	(u256)[input_0 + 768] = x3;
	(u256)[input_0 + 1024] = x4;
	(u256)[input_0 + 1280] = x5;
	(u256)[input_0 + 1536] = x6;
	(u256)[input_0 + 1792] = x7;
	x0 = (u256)[input_0 + 32];
	x1 = (u256)[input_0 + 288];
	x2 = (u256)[input_0 + 544];
	x3 = (u256)[input_0 + 800];
	x4 = (u256)[input_0 + 1056];
	x5 = (u256)[input_0 + 1312];
	x6 = (u256)[input_0 + 1568];
	x7 = (u256)[input_0 + 1824];
	v00 = x0 & mask0;
	v10 = x4 <<4u64 32;
	v01 = x0 >>4u64 32;
	v11 = x4 & mask1;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 <<4u64 32;
	v01 = x1 >>4u64 32;
	v11 = x5 & mask1;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 <<4u64 32;
	v01 = x2 >>4u64 32;
	v11 = x6 & mask1;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 <<4u64 32;
	v01 = x3 >>4u64 32;
	v11 = x7 & mask1;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 <<8u32 16;
	v01 = x0 >>8u32 16;
	v11 = x2 & mask3;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 <<8u32 16;
	v01 = x1 >>8u32 16;
	v11 = x3 & mask3;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 <<8u32 16;
	v01 = x4 >>8u32 16;
	v11 = x6 & mask3;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 <<8u32 16;
	v01 = x5 >>8u32 16;
	v11 = x7 & mask3;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 <<16u16 8;
	v01 = x0 >>16u16 8;
	v11 = x1 & mask5;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 <<16u16 8;
	v01 = x2 >>16u16 8;
	v11 = x3 & mask5;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 <<16u16 8;
	v01 = x4 >>16u16 8;
	v11 = x5 & mask5;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 <<16u16 8;
	v01 = x6 >>16u16 8;
	v11 = x7 & mask5;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 32] = x0;
	(u256)[input_0 + 288] = x1;
	(u256)[input_0 + 544] = x2;
	(u256)[input_0 + 800] = x3;
	(u256)[input_0 + 1056] = x4;
	(u256)[input_0 + 1312] = x5;
	(u256)[input_0 + 1568] = x6;
	(u256)[input_0 + 1824] = x7;
	x0 = (u256)[input_0 + 64];
	x1 = (u256)[input_0 + 320];
	x2 = (u256)[input_0 + 576];
	x3 = (u256)[input_0 + 832];
	x4 = (u256)[input_0 + 1088];
	x5 = (u256)[input_0 + 1344];
	x6 = (u256)[input_0 + 1600];
	x7 = (u256)[input_0 + 1856];
	v00 = x0 & mask0;
	v10 = x4 <<4u64 32;
	v01 = x0 >>4u64 32;
	v11 = x4 & mask1;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 <<4u64 32;
	v01 = x1 >>4u64 32;
	v11 = x5 & mask1;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 <<4u64 32;
	v01 = x2 >>4u64 32;
	v11 = x6 & mask1;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 <<4u64 32;
	v01 = x3 >>4u64 32;
	v11 = x7 & mask1;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 <<8u32 16;
	v01 = x0 >>8u32 16;
	v11 = x2 & mask3;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 <<8u32 16;
	v01 = x1 >>8u32 16;
	v11 = x3 & mask3;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 <<8u32 16;
	v01 = x4 >>8u32 16;
	v11 = x6 & mask3;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 <<8u32 16;
	v01 = x5 >>8u32 16;
	v11 = x7 & mask3;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 <<16u16 8;
	v01 = x0 >>16u16 8;
	v11 = x1 & mask5;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 <<16u16 8;
	v01 = x2 >>16u16 8;
	v11 = x3 & mask5;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 <<16u16 8;
	v01 = x4 >>16u16 8;
	v11 = x5 & mask5;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 <<16u16 8;
	v01 = x6 >>16u16 8;
	v11 = x7 & mask5;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 64] = x0;
	(u256)[input_0 + 320] = x1;
	(u256)[input_0 + 576] = x2;
	(u256)[input_0 + 832] = x3;
	(u256)[input_0 + 1088] = x4;
	(u256)[input_0 + 1344] = x5;
	(u256)[input_0 + 1600] = x6;
	(u256)[input_0 + 1856] = x7;
	x0 = (u256)[input_0 + 96];
	x1 = (u256)[input_0 + 352];
	x2 = (u256)[input_0 + 608];
	x3 = (u256)[input_0 + 864];
	x4 = (u256)[input_0 + 1120];
	x5 = (u256)[input_0 + 1376];
	x6 = (u256)[input_0 + 1632];
	x7 = (u256)[input_0 + 1888];
	v00 = x0 & mask0;
	v10 = x4 <<4u64 32;
	v01 = x0 >>4u64 32;
	v11 = x4 & mask1;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 <<4u64 32;
	v01 = x1 >>4u64 32;
	v11 = x5 & mask1;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 <<4u64 32;
	v01 = x2 >>4u64 32;
	v11 = x6 & mask1;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 <<4u64 32;
	v01 = x3 >>4u64 32;
	v11 = x7 & mask1;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 <<8u32 16;
	v01 = x0 >>8u32 16;
	v11 = x2 & mask3;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 <<8u32 16;
	v01 = x1 >>8u32 16;
	v11 = x3 & mask3;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 <<8u32 16;
	v01 = x4 >>8u32 16;
	v11 = x6 & mask3;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 <<8u32 16;
	v01 = x5 >>8u32 16;
	v11 = x7 & mask3;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 <<16u16 8;
	v01 = x0 >>16u16 8;
	v11 = x1 & mask5;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 <<16u16 8;
	v01 = x2 >>16u16 8;
	v11 = x3 & mask5;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 <<16u16 8;
	v01 = x4 >>16u16 8;
	v11 = x5 & mask5;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 <<16u16 8;
	v01 = x6 >>16u16 8;
	v11 = x7 & mask5;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 96] = x0;
	(u256)[input_0 + 352] = x1;
	(u256)[input_0 + 608] = x2;
	(u256)[input_0 + 864] = x3;
	(u256)[input_0 + 1120] = x4;
	(u256)[input_0 + 1376] = x5;
	(u256)[input_0 + 1632] = x6;
	(u256)[input_0 + 1888] = x7;
	x0 = (u256)[input_0 + 128];
	x1 = (u256)[input_0 + 384];
	x2 = (u256)[input_0 + 640];
	x3 = (u256)[input_0 + 896];
	x4 = (u256)[input_0 + 1152];
	x5 = (u256)[input_0 + 1408];
	x6 = (u256)[input_0 + 1664];
	x7 = (u256)[input_0 + 1920];
	v00 = x0 & mask0;
	v10 = x4 <<4u64 32;
	v01 = x0 >>4u64 32;
	v11 = x4 & mask1;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 <<4u64 32;
	v01 = x1 >>4u64 32;
	v11 = x5 & mask1;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 <<4u64 32;
	v01 = x2 >>4u64 32;
	v11 = x6 & mask1;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 <<4u64 32;
	v01 = x3 >>4u64 32;
	v11 = x7 & mask1;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 <<8u32 16;
	v01 = x0 >>8u32 16;
	v11 = x2 & mask3;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 <<8u32 16;
	v01 = x1 >>8u32 16;
	v11 = x3 & mask3;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 <<8u32 16;
	v01 = x4 >>8u32 16;
	v11 = x6 & mask3;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 <<8u32 16;
	v01 = x5 >>8u32 16;
	v11 = x7 & mask3;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 <<16u16 8;
	v01 = x0 >>16u16 8;
	v11 = x1 & mask5;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 <<16u16 8;
	v01 = x2 >>16u16 8;
	v11 = x3 & mask5;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 <<16u16 8;
	v01 = x4 >>16u16 8;
	v11 = x5 & mask5;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 <<16u16 8;
	v01 = x6 >>16u16 8;
	v11 = x7 & mask5;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 128] = x0;
	(u256)[input_0 + 384] = x1;
	(u256)[input_0 + 640] = x2;
	(u256)[input_0 + 896] = x3;
	(u256)[input_0 + 1152] = x4;
	(u256)[input_0 + 1408] = x5;
	(u256)[input_0 + 1664] = x6;
	(u256)[input_0 + 1920] = x7;
	x0 = (u256)[input_0 + 160];
	x1 = (u256)[input_0 + 416];
	x2 = (u256)[input_0 + 672];
	x3 = (u256)[input_0 + 928];
	x4 = (u256)[input_0 + 1184];
	x5 = (u256)[input_0 + 1440];
	x6 = (u256)[input_0 + 1696];
	x7 = (u256)[input_0 + 1952];
	v00 = x0 & mask0;
	v10 = x4 <<4u64 32;
	v01 = x0 >>4u64 32;
	v11 = x4 & mask1;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 <<4u64 32;
	v01 = x1 >>4u64 32;
	v11 = x5 & mask1;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 <<4u64 32;
	v01 = x2 >>4u64 32;
	v11 = x6 & mask1;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 <<4u64 32;
	v01 = x3 >>4u64 32;
	v11 = x7 & mask1;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 <<8u32 16;
	v01 = x0 >>8u32 16;
	v11 = x2 & mask3;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 <<8u32 16;
	v01 = x1 >>8u32 16;
	v11 = x3 & mask3;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 <<8u32 16;
	v01 = x4 >>8u32 16;
	v11 = x6 & mask3;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 <<8u32 16;
	v01 = x5 >>8u32 16;
	v11 = x7 & mask3;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 <<16u16 8;
	v01 = x0 >>16u16 8;
	v11 = x1 & mask5;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 <<16u16 8;
	v01 = x2 >>16u16 8;
	v11 = x3 & mask5;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 <<16u16 8;
	v01 = x4 >>16u16 8;
	v11 = x5 & mask5;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 <<16u16 8;
	v01 = x6 >>16u16 8;
	v11 = x7 & mask5;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 160] = x0;
	(u256)[input_0 + 416] = x1;
	(u256)[input_0 + 672] = x2;
	(u256)[input_0 + 928] = x3;
	(u256)[input_0 + 1184] = x4;
	(u256)[input_0 + 1440] = x5;
	(u256)[input_0 + 1696] = x6;
	(u256)[input_0 + 1952] = x7;
	x0 = (u256)[input_0 + 192];
	x1 = (u256)[input_0 + 448];
	x2 = (u256)[input_0 + 704];
	x3 = (u256)[input_0 + 960];
	x4 = (u256)[input_0 + 1216];
	x5 = (u256)[input_0 + 1472];
	x6 = (u256)[input_0 + 1728];
	x7 = (u256)[input_0 + 1984];
	v00 = x0 & mask0;
	v10 = x4 <<4u64 32;
	v01 = x0 >>4u64 32;
	v11 = x4 & mask1;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 <<4u64 32;
	v01 = x1 >>4u64 32;
	v11 = x5 & mask1;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 <<4u64 32;
	v01 = x2 >>4u64 32;
	v11 = x6 & mask1;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 <<4u64 32;
	v01 = x3 >>4u64 32;
	v11 = x7 & mask1;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 <<8u32 16;
	v01 = x0 >>8u32 16;
	v11 = x2 & mask3;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 <<8u32 16;
	v01 = x1 >>8u32 16;
	v11 = x3 & mask3;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 <<8u32 16;
	v01 = x4 >>8u32 16;
	v11 = x6 & mask3;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 <<8u32 16;
	v01 = x5 >>8u32 16;
	v11 = x7 & mask3;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 <<16u16 8;
	v01 = x0 >>16u16 8;
	v11 = x1 & mask5;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 <<16u16 8;
	v01 = x2 >>16u16 8;
	v11 = x3 & mask5;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 <<16u16 8;
	v01 = x4 >>16u16 8;
	v11 = x5 & mask5;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 <<16u16 8;
	v01 = x6 >>16u16 8;
	v11 = x7 & mask5;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 192] = x0;
	(u256)[input_0 + 448] = x1;
	(u256)[input_0 + 704] = x2;
	(u256)[input_0 + 960] = x3;
	(u256)[input_0 + 1216] = x4;
	(u256)[input_0 + 1472] = x5;
	(u256)[input_0 + 1728] = x6;
	(u256)[input_0 + 1984] = x7;
	x0 = (u256)[input_0 + 224];
	x1 = (u256)[input_0 + 480];
	x2 = (u256)[input_0 + 736];
	x3 = (u256)[input_0 + 992];
	x4 = (u256)[input_0 + 1248];
	x5 = (u256)[input_0 + 1504];
	x6 = (u256)[input_0 + 1760];
	x7 = (u256)[input_0 + 2016];
	v00 = x0 & mask0;
	v10 = x4 <<4u64 32;
	v01 = x0 >>4u64 32;
	v11 = x4 & mask1;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 <<4u64 32;
	v01 = x1 >>4u64 32;
	v11 = x5 & mask1;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 <<4u64 32;
	v01 = x2 >>4u64 32;
	v11 = x6 & mask1;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 <<4u64 32;
	v01 = x3 >>4u64 32;
	v11 = x7 & mask1;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 <<8u32 16;
	v01 = x0 >>8u32 16;
	v11 = x2 & mask3;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 <<8u32 16;
	v01 = x1 >>8u32 16;
	v11 = x3 & mask3;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 <<8u32 16;
	v01 = x4 >>8u32 16;
	v11 = x6 & mask3;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 <<8u32 16;
	v01 = x5 >>8u32 16;
	v11 = x7 & mask3;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 <<16u16 8;
	v01 = x0 >>16u16 8;
	v11 = x1 & mask5;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 <<16u16 8;
	v01 = x2 >>16u16 8;
	v11 = x3 & mask5;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 <<16u16 8;
	v01 = x4 >>16u16 8;
	v11 = x5 & mask5;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 <<16u16 8;
	v01 = x6 >>16u16 8;
	v11 = x7 & mask5;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 224] = x0;
	(u256)[input_0 + 480] = x1;
	(u256)[input_0 + 736] = x2;
	(u256)[input_0 + 992] = x3;
	(u256)[input_0 + 1248] = x4;
	(u256)[input_0 + 1504] = x5;
	(u256)[input_0 + 1760] = x6;
	(u256)[input_0 + 2016] = x7;
	mask0 = MASK2_0;
	mask1 = MASK2_1;
	mask2 = MASK1_0;
	mask3 = MASK1_1;
	mask4 = MASK0_0;
	mask5 = MASK0_1;
	x0 = (u256)[input_0 + 0];
	x1 = (u256)[input_0 + 32];
	x2 = (u256)[input_0 + 64];
	x3 = (u256)[input_0 + 96];
	x4 = (u256)[input_0 + 128];
	x5 = (u256)[input_0 + 160];
	x6 = (u256)[input_0 + 192];
	x7 = (u256)[input_0 + 224];
	v00 = x0 & mask0;
	v10 = x4 & mask0;
	v10 <<4u64= 4;
	v01 = x0 & mask1;
	v11 = x4 & mask1;
	v01 >>4u64= 4;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 & mask0;
	v10 <<4u64= 4;
	v01 = x1 & mask1;
	v11 = x5 & mask1;
	v01 >>4u64= 4;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 & mask0;
	v10 <<4u64= 4;
	v01 = x2 & mask1;
	v11 = x6 & mask1;
	v01 >>4u64= 4;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 & mask0;
	v10 <<4u64= 4;
	v01 = x3 & mask1;
	v11 = x7 & mask1;
	v01 >>4u64= 4;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 & mask2;
	v10 <<4u64= 2;
	v01 = x0 & mask3;
	v11 = x2 & mask3;
	v01 >>4u64= 2;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 & mask2;
	v10 <<4u64= 2;
	v01 = x1 & mask3;
	v11 = x3 & mask3;
	v01 >>4u64= 2;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 & mask2;
	v10 <<4u64= 2;
	v01 = x4 & mask3;
	v11 = x6 & mask3;
	v01 >>4u64= 2;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 & mask2;
	v10 <<4u64= 2;
	v01 = x5 & mask3;
	v11 = x7 & mask3;
	v01 >>4u64= 2;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 & mask4;
	v10 <<4u64= 1;
	v01 = x0 & mask5;
	v11 = x1 & mask5;
	v01 >>4u64= 1;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 & mask4;
	v10 <<4u64= 1;
	v01 = x2 & mask5;
	v11 = x3 & mask5;
	v01 >>4u64= 1;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 & mask4;
	v10 <<4u64= 1;
	v01 = x4 & mask5;
	v11 = x5 & mask5;
	v01 >>4u64= 1;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 & mask4;
	v10 <<4u64= 1;
	v01 = x6 & mask5;
	v11 = x7 & mask5;
	v01 >>4u64= 1;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 0] = x0;
	(u256)[input_0 + 32] = x1;
	(u256)[input_0 + 64] = x2;
	(u256)[input_0 + 96] = x3;
	(u256)[input_0 + 128] = x4;
	(u256)[input_0 + 160] = x5;
	(u256)[input_0 + 192] = x6;
	(u256)[input_0 + 224] = x7;
	x0 = (u256)[input_0 + 256];
	x1 = (u256)[input_0 + 288];
	x2 = (u256)[input_0 + 320];
	x3 = (u256)[input_0 + 352];
	x4 = (u256)[input_0 + 384];
	x5 = (u256)[input_0 + 416];
	x6 = (u256)[input_0 + 448];
	x7 = (u256)[input_0 + 480];
	v00 = x0 & mask0;
	v10 = x4 & mask0;
	v10 <<4u64= 4;
	v01 = x0 & mask1;
	v11 = x4 & mask1;
	v01 >>4u64= 4;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 & mask0;
	v10 <<4u64= 4;
	v01 = x1 & mask1;
	v11 = x5 & mask1;
	v01 >>4u64= 4;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 & mask0;
	v10 <<4u64= 4;
	v01 = x2 & mask1;
	v11 = x6 & mask1;
	v01 >>4u64= 4;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 & mask0;
	v10 <<4u64= 4;
	v01 = x3 & mask1;
	v11 = x7 & mask1;
	v01 >>4u64= 4;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 & mask2;
	v10 <<4u64= 2;
	v01 = x0 & mask3;
	v11 = x2 & mask3;
	v01 >>4u64= 2;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 & mask2;
	v10 <<4u64= 2;
	v01 = x1 & mask3;
	v11 = x3 & mask3;
	v01 >>4u64= 2;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 & mask2;
	v10 <<4u64= 2;
	v01 = x4 & mask3;
	v11 = x6 & mask3;
	v01 >>4u64= 2;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 & mask2;
	v10 <<4u64= 2;
	v01 = x5 & mask3;
	v11 = x7 & mask3;
	v01 >>4u64= 2;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 & mask4;
	v10 <<4u64= 1;
	v01 = x0 & mask5;
	v11 = x1 & mask5;
	v01 >>4u64= 1;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 & mask4;
	v10 <<4u64= 1;
	v01 = x2 & mask5;
	v11 = x3 & mask5;
	v01 >>4u64= 1;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 & mask4;
	v10 <<4u64= 1;
	v01 = x4 & mask5;
	v11 = x5 & mask5;
	v01 >>4u64= 1;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 & mask4;
	v10 <<4u64= 1;
	v01 = x6 & mask5;
	v11 = x7 & mask5;
	v01 >>4u64= 1;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 256] = x0;
	(u256)[input_0 + 288] = x1;
	(u256)[input_0 + 320] = x2;
	(u256)[input_0 + 352] = x3;
	(u256)[input_0 + 384] = x4;
	(u256)[input_0 + 416] = x5;
	(u256)[input_0 + 448] = x6;
	(u256)[input_0 + 480] = x7;
	x0 = (u256)[input_0 + 512];
	x1 = (u256)[input_0 + 544];
	x2 = (u256)[input_0 + 576];
	x3 = (u256)[input_0 + 608];
	x4 = (u256)[input_0 + 640];
	x5 = (u256)[input_0 + 672];
	x6 = (u256)[input_0 + 704];
	x7 = (u256)[input_0 + 736];
	v00 = x0 & mask0;
	v10 = x4 & mask0;
	v10 <<4u64= 4;
	v01 = x0 & mask1;
	v11 = x4 & mask1;
	v01 >>4u64= 4;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 & mask0;
	v10 <<4u64= 4;
	v01 = x1 & mask1;
	v11 = x5 & mask1;
	v01 >>4u64= 4;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 & mask0;
	v10 <<4u64= 4;
	v01 = x2 & mask1;
	v11 = x6 & mask1;
	v01 >>4u64= 4;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 & mask0;
	v10 <<4u64= 4;
	v01 = x3 & mask1;
	v11 = x7 & mask1;
	v01 >>4u64= 4;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 & mask2;
	v10 <<4u64= 2;
	v01 = x0 & mask3;
	v11 = x2 & mask3;
	v01 >>4u64= 2;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 & mask2;
	v10 <<4u64= 2;
	v01 = x1 & mask3;
	v11 = x3 & mask3;
	v01 >>4u64= 2;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 & mask2;
	v10 <<4u64= 2;
	v01 = x4 & mask3;
	v11 = x6 & mask3;
	v01 >>4u64= 2;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 & mask2;
	v10 <<4u64= 2;
	v01 = x5 & mask3;
	v11 = x7 & mask3;
	v01 >>4u64= 2;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 & mask4;
	v10 <<4u64= 1;
	v01 = x0 & mask5;
	v11 = x1 & mask5;
	v01 >>4u64= 1;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 & mask4;
	v10 <<4u64= 1;
	v01 = x2 & mask5;
	v11 = x3 & mask5;
	v01 >>4u64= 1;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 & mask4;
	v10 <<4u64= 1;
	v01 = x4 & mask5;
	v11 = x5 & mask5;
	v01 >>4u64= 1;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 & mask4;
	v10 <<4u64= 1;
	v01 = x6 & mask5;
	v11 = x7 & mask5;
	v01 >>4u64= 1;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 512] = x0;
	(u256)[input_0 + 544] = x1;
	(u256)[input_0 + 576] = x2;
	(u256)[input_0 + 608] = x3;
	(u256)[input_0 + 640] = x4;
	(u256)[input_0 + 672] = x5;
	(u256)[input_0 + 704] = x6;
	(u256)[input_0 + 736] = x7;
	x0 = (u256)[input_0 + 768];
	x1 = (u256)[input_0 + 800];
	x2 = (u256)[input_0 + 832];
	x3 = (u256)[input_0 + 864];
	x4 = (u256)[input_0 + 896];
	x5 = (u256)[input_0 + 928];
	x6 = (u256)[input_0 + 960];
	x7 = (u256)[input_0 + 992];
	v00 = x0 & mask0;
	v10 = x4 & mask0;
	v10 <<4u64= 4;
	v01 = x0 & mask1;
	v11 = x4 & mask1;
	v01 >>4u64= 4;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 & mask0;
	v10 <<4u64= 4;
	v01 = x1 & mask1;
	v11 = x5 & mask1;
	v01 >>4u64= 4;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 & mask0;
	v10 <<4u64= 4;
	v01 = x2 & mask1;
	v11 = x6 & mask1;
	v01 >>4u64= 4;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 & mask0;
	v10 <<4u64= 4;
	v01 = x3 & mask1;
	v11 = x7 & mask1;
	v01 >>4u64= 4;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 & mask2;
	v10 <<4u64= 2;
	v01 = x0 & mask3;
	v11 = x2 & mask3;
	v01 >>4u64= 2;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 & mask2;
	v10 <<4u64= 2;
	v01 = x1 & mask3;
	v11 = x3 & mask3;
	v01 >>4u64= 2;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 & mask2;
	v10 <<4u64= 2;
	v01 = x4 & mask3;
	v11 = x6 & mask3;
	v01 >>4u64= 2;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 & mask2;
	v10 <<4u64= 2;
	v01 = x5 & mask3;
	v11 = x7 & mask3;
	v01 >>4u64= 2;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 & mask4;
	v10 <<4u64= 1;
	v01 = x0 & mask5;
	v11 = x1 & mask5;
	v01 >>4u64= 1;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 & mask4;
	v10 <<4u64= 1;
	v01 = x2 & mask5;
	v11 = x3 & mask5;
	v01 >>4u64= 1;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 & mask4;
	v10 <<4u64= 1;
	v01 = x4 & mask5;
	v11 = x5 & mask5;
	v01 >>4u64= 1;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 & mask4;
	v10 <<4u64= 1;
	v01 = x6 & mask5;
	v11 = x7 & mask5;
	v01 >>4u64= 1;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 768] = x0;
	(u256)[input_0 + 800] = x1;
	(u256)[input_0 + 832] = x2;
	(u256)[input_0 + 864] = x3;
	(u256)[input_0 + 896] = x4;
	(u256)[input_0 + 928] = x5;
	(u256)[input_0 + 960] = x6;
	(u256)[input_0 + 992] = x7;
	x0 = (u256)[input_0 + 1024];
	x1 = (u256)[input_0 + 1056];
	x2 = (u256)[input_0 + 1088];
	x3 = (u256)[input_0 + 1120];
	x4 = (u256)[input_0 + 1152];
	x5 = (u256)[input_0 + 1184];
	x6 = (u256)[input_0 + 1216];
	x7 = (u256)[input_0 + 1248];
	v00 = x0 & mask0;
	v10 = x4 & mask0;
	v10 <<4u64= 4;
	v01 = x0 & mask1;
	v11 = x4 & mask1;
	v01 >>4u64= 4;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 & mask0;
	v10 <<4u64= 4;
	v01 = x1 & mask1;
	v11 = x5 & mask1;
	v01 >>4u64= 4;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 & mask0;
	v10 <<4u64= 4;
	v01 = x2 & mask1;
	v11 = x6 & mask1;
	v01 >>4u64= 4;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 & mask0;
	v10 <<4u64= 4;
	v01 = x3 & mask1;
	v11 = x7 & mask1;
	v01 >>4u64= 4;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 & mask2;
	v10 <<4u64= 2;
	v01 = x0 & mask3;
	v11 = x2 & mask3;
	v01 >>4u64= 2;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 & mask2;
	v10 <<4u64= 2;
	v01 = x1 & mask3;
	v11 = x3 & mask3;
	v01 >>4u64= 2;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 & mask2;
	v10 <<4u64= 2;
	v01 = x4 & mask3;
	v11 = x6 & mask3;
	v01 >>4u64= 2;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 & mask2;
	v10 <<4u64= 2;
	v01 = x5 & mask3;
	v11 = x7 & mask3;
	v01 >>4u64= 2;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 & mask4;
	v10 <<4u64= 1;
	v01 = x0 & mask5;
	v11 = x1 & mask5;
	v01 >>4u64= 1;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 & mask4;
	v10 <<4u64= 1;
	v01 = x2 & mask5;
	v11 = x3 & mask5;
	v01 >>4u64= 1;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 & mask4;
	v10 <<4u64= 1;
	v01 = x4 & mask5;
	v11 = x5 & mask5;
	v01 >>4u64= 1;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 & mask4;
	v10 <<4u64= 1;
	v01 = x6 & mask5;
	v11 = x7 & mask5;
	v01 >>4u64= 1;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 1024] = x0;
	(u256)[input_0 + 1056] = x1;
	(u256)[input_0 + 1088] = x2;
	(u256)[input_0 + 1120] = x3;
	(u256)[input_0 + 1152] = x4;
	(u256)[input_0 + 1184] = x5;
	(u256)[input_0 + 1216] = x6;
	(u256)[input_0 + 1248] = x7;
	x0 = (u256)[input_0 + 1280];
	x1 = (u256)[input_0 + 1312];
	x2 = (u256)[input_0 + 1344];
	x3 = (u256)[input_0 + 1376];
	x4 = (u256)[input_0 + 1408];
	x5 = (u256)[input_0 + 1440];
	x6 = (u256)[input_0 + 1472];
	x7 = (u256)[input_0 + 1504];
	v00 = x0 & mask0;
	v10 = x4 & mask0;
	v10 <<4u64= 4;
	v01 = x0 & mask1;
	v11 = x4 & mask1;
	v01 >>4u64= 4;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 & mask0;
	v10 <<4u64= 4;
	v01 = x1 & mask1;
	v11 = x5 & mask1;
	v01 >>4u64= 4;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 & mask0;
	v10 <<4u64= 4;
	v01 = x2 & mask1;
	v11 = x6 & mask1;
	v01 >>4u64= 4;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 & mask0;
	v10 <<4u64= 4;
	v01 = x3 & mask1;
	v11 = x7 & mask1;
	v01 >>4u64= 4;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 & mask2;
	v10 <<4u64= 2;
	v01 = x0 & mask3;
	v11 = x2 & mask3;
	v01 >>4u64= 2;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 & mask2;
	v10 <<4u64= 2;
	v01 = x1 & mask3;
	v11 = x3 & mask3;
	v01 >>4u64= 2;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 & mask2;
	v10 <<4u64= 2;
	v01 = x4 & mask3;
	v11 = x6 & mask3;
	v01 >>4u64= 2;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 & mask2;
	v10 <<4u64= 2;
	v01 = x5 & mask3;
	v11 = x7 & mask3;
	v01 >>4u64= 2;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 & mask4;
	v10 <<4u64= 1;
	v01 = x0 & mask5;
	v11 = x1 & mask5;
	v01 >>4u64= 1;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 & mask4;
	v10 <<4u64= 1;
	v01 = x2 & mask5;
	v11 = x3 & mask5;
	v01 >>4u64= 1;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 & mask4;
	v10 <<4u64= 1;
	v01 = x4 & mask5;
	v11 = x5 & mask5;
	v01 >>4u64= 1;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 & mask4;
	v10 <<4u64= 1;
	v01 = x6 & mask5;
	v11 = x7 & mask5;
	v01 >>4u64= 1;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 1280] = x0;
	(u256)[input_0 + 1312] = x1;
	(u256)[input_0 + 1344] = x2;
	(u256)[input_0 + 1376] = x3;
	(u256)[input_0 + 1408] = x4;
	(u256)[input_0 + 1440] = x5;
	(u256)[input_0 + 1472] = x6;
	(u256)[input_0 + 1504] = x7;
	x0 = (u256)[input_0 + 1536];
	x1 = (u256)[input_0 + 1568];
	x2 = (u256)[input_0 + 1600];
	x3 = (u256)[input_0 + 1632];
	x4 = (u256)[input_0 + 1664];
	x5 = (u256)[input_0 + 1696];
	x6 = (u256)[input_0 + 1728];
	x7 = (u256)[input_0 + 1760];
	v00 = x0 & mask0;
	v10 = x4 & mask0;
	v10 <<4u64= 4;
	v01 = x0 & mask1;
	v11 = x4 & mask1;
	v01 >>4u64= 4;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 & mask0;
	v10 <<4u64= 4;
	v01 = x1 & mask1;
	v11 = x5 & mask1;
	v01 >>4u64= 4;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 & mask0;
	v10 <<4u64= 4;
	v01 = x2 & mask1;
	v11 = x6 & mask1;
	v01 >>4u64= 4;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 & mask0;
	v10 <<4u64= 4;
	v01 = x3 & mask1;
	v11 = x7 & mask1;
	v01 >>4u64= 4;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 & mask2;
	v10 <<4u64= 2;
	v01 = x0 & mask3;
	v11 = x2 & mask3;
	v01 >>4u64= 2;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 & mask2;
	v10 <<4u64= 2;
	v01 = x1 & mask3;
	v11 = x3 & mask3;
	v01 >>4u64= 2;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 & mask2;
	v10 <<4u64= 2;
	v01 = x4 & mask3;
	v11 = x6 & mask3;
	v01 >>4u64= 2;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 & mask2;
	v10 <<4u64= 2;
	v01 = x5 & mask3;
	v11 = x7 & mask3;
	v01 >>4u64= 2;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 & mask4;
	v10 <<4u64= 1;
	v01 = x0 & mask5;
	v11 = x1 & mask5;
	v01 >>4u64= 1;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 & mask4;
	v10 <<4u64= 1;
	v01 = x2 & mask5;
	v11 = x3 & mask5;
	v01 >>4u64= 1;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 & mask4;
	v10 <<4u64= 1;
	v01 = x4 & mask5;
	v11 = x5 & mask5;
	v01 >>4u64= 1;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 & mask4;
	v10 <<4u64= 1;
	v01 = x6 & mask5;
	v11 = x7 & mask5;
	v01 >>4u64= 1;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 1536] = x0;
	(u256)[input_0 + 1568] = x1;
	(u256)[input_0 + 1600] = x2;
	(u256)[input_0 + 1632] = x3;
	(u256)[input_0 + 1664] = x4;
	(u256)[input_0 + 1696] = x5;
	(u256)[input_0 + 1728] = x6;
	(u256)[input_0 + 1760] = x7;
	x0 = (u256)[input_0 + 1792];
	x1 = (u256)[input_0 + 1824];
	x2 = (u256)[input_0 + 1856];
	x3 = (u256)[input_0 + 1888];
	x4 = (u256)[input_0 + 1920];
	x5 = (u256)[input_0 + 1952];
	x6 = (u256)[input_0 + 1984];
	x7 = (u256)[input_0 + 2016];
	v00 = x0 & mask0;
	v10 = x4 & mask0;
	v10 <<4u64= 4;
	v01 = x0 & mask1;
	v11 = x4 & mask1;
	v01 >>4u64= 4;
	x0 = v00 | v10;
	x4 = v01 | v11;
	v00 = x1 & mask0;
	v10 = x5 & mask0;
	v10 <<4u64= 4;
	v01 = x1 & mask1;
	v11 = x5 & mask1;
	v01 >>4u64= 4;
	x1 = v00 | v10;
	x5 = v01 | v11;
	v00 = x2 & mask0;
	v10 = x6 & mask0;
	v10 <<4u64= 4;
	v01 = x2 & mask1;
	v11 = x6 & mask1;
	v01 >>4u64= 4;
	x2 = v00 | v10;
	x6 = v01 | v11;
	v00 = x3 & mask0;
	v10 = x7 & mask0;
	v10 <<4u64= 4;
	v01 = x3 & mask1;
	v11 = x7 & mask1;
	v01 >>4u64= 4;
	x3 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask2;
	v10 = x2 & mask2;
	v10 <<4u64= 2;
	v01 = x0 & mask3;
	v11 = x2 & mask3;
	v01 >>4u64= 2;
	x0 = v00 | v10;
	x2 = v01 | v11;
	v00 = x1 & mask2;
	v10 = x3 & mask2;
	v10 <<4u64= 2;
	v01 = x1 & mask3;
	v11 = x3 & mask3;
	v01 >>4u64= 2;
	x1 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask2;
	v10 = x6 & mask2;
	v10 <<4u64= 2;
	v01 = x4 & mask3;
	v11 = x6 & mask3;
	v01 >>4u64= 2;
	x4 = v00 | v10;
	x6 = v01 | v11;
	v00 = x5 & mask2;
	v10 = x7 & mask2;
	v10 <<4u64= 2;
	v01 = x5 & mask3;
	v11 = x7 & mask3;
	v01 >>4u64= 2;
	x5 = v00 | v10;
	x7 = v01 | v11;
	v00 = x0 & mask4;
	v10 = x1 & mask4;
	v10 <<4u64= 1;
	v01 = x0 & mask5;
	v11 = x1 & mask5;
	v01 >>4u64= 1;
	x0 = v00 | v10;
	x1 = v01 | v11;
	v00 = x2 & mask4;
	v10 = x3 & mask4;
	v10 <<4u64= 1;
	v01 = x2 & mask5;
	v11 = x3 & mask5;
	v01 >>4u64= 1;
	x2 = v00 | v10;
	x3 = v01 | v11;
	v00 = x4 & mask4;
	v10 = x5 & mask4;
	v10 <<4u64= 1;
	v01 = x4 & mask5;
	v11 = x5 & mask5;
	v01 >>4u64= 1;
	x4 = v00 | v10;
	x5 = v01 | v11;
	v00 = x6 & mask4;
	v10 = x7 & mask4;
	v10 <<4u64= 1;
	v01 = x6 & mask5;
	v11 = x7 & mask5;
	v01 >>4u64= 1;
	x6 = v00 | v10;
	x7 = v01 | v11;
	(u256)[input_0 + 1792] = x0;
	(u256)[input_0 + 1824] = x1;
	(u256)[input_0 + 1856] = x2;
	(u256)[input_0 + 1888] = x3;
	(u256)[input_0 + 1920] = x4;
	(u256)[input_0 + 1952] = x5;
	(u256)[input_0 + 1984] = x6;
	(u256)[input_0 + 2016] = x7;
	return;
}


