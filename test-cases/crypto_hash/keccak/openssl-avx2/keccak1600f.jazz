export fn SHA3_absorb(){

	// UNMATCHED: mov %rsp,%r11

	// UNMATCHED: lea -240(%rsp),%rsp

	// UNMATCHED: and $-32,%rsp

	// UNMATCHED: lea 96(a_flat),a_flat

	// UNMATCHED: lea 96(inp),inp

	// UNMATCHED: lea 96(%rsp),a_stack

	// UNMATCHED: vzeroupper

	// UNMATCHED: vpbroadcastq -96(a_flat),state[0] 

	state[1] = B256(a_flat,0,(96-8));
	state[2] = B256(a_flat,1,(96-8));
	state[3] = B256(a_flat,2,(96-8));
	state[4] = B256(a_flat,3,(96-8));
	state[5] = B256(a_flat,4,(96-8));
	state[6] = B256(a_flat,5,(96-8));
	t[0] = t[0] ^ t[0];
	B256(a_stack,2,96) = t[0];
	B256(a_stack,3,96) = t[0];
	B256(a_stack,4,96) = t[0];
	B256(a_stack,5,96) = t[0];
	B256(a_stack,6,96) = t[0];
	// UNMATCHED: .Loop_absorb_avx2:

	// UNMATCHED: mov bsz,%rax

	// UNMATCHED: sub bsz,len

	// UNMATCHED: jc .Ldone_absorb_avx2

	// UNMATCHED: shr $3,%eax

	// UNMATCHED: vpbroadcastq 0-96(inp),t[0]

	// UNMATCHED: vmovdqu 8-96(inp),t[1]

	// UNMATCHED: sub $4,%eax

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*5-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(2,2)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*6-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(6,0)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*7-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(3,1)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*8-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(4,2)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*9-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(5,3)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*10-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(2,0)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*11-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(4,0)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*12-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(6,1)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*13-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(5,2)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*14-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(3,3)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*15-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(2,3)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*16-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(3,0)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*17-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(5,1)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*18-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(6,2)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*19-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(4,3)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*20-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(2,1)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*21-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(5,0)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*22-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(4,1)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*23-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(3,2)-96(a_stack)

	// UNMATCHED: dec %eax

	// UNMATCHED: jz .Labsorved_avx2

	// UNMATCHED: mov 8*24-96(inp),%r8

	// UNMATCHED: mov %r8,a_jagged(6,3)-96(a_stack)

	// UNMATCHED: .Labsorved_avx2:

	// UNMATCHED: lea (inp,bsz),inp

	state[0] = state[0] ^ t[0];
	state[1] = state[1] ^ t[1];
	state[2] = state[2] ^ B256(a_stack,2,96);
	state[3] = state[3] ^ B256(a_stack,3,96);
	state[4] = state[4] ^ B256(a_stack,4,96);
	state[5] = state[5] ^ B256(a_stack,5,96);
	state[6] = state[6] ^ B256(a_stack,6,96);
	// UNMATCHED: call __KeccakF1600

	// UNMATCHED: lea 96(%rsp),a_stack

	// UNMATCHED: jmp .Loop_absorb_avx2

	// UNMATCHED: .Ldone_absorb_avx2:

	// UNMATCHED: vmovq %xmm0,-96(a_flat) 

	B256(a_flat,0,(96-8)) = state[1];
	B256(a_flat,1,(96-8)) = state[2];
	B256(a_flat,2,(96-8)) = state[3];
	B256(a_flat,3,(96-8)) = state[4];
	B256(a_flat,4,(96-8)) = state[5];
	B256(a_flat,5,(96-8)) = state[6];
	// UNMATCHED: vzeroupper

	// UNMATCHED: lea (%r11),%rsp

	// UNMATCHED: lea (len,bsz),%rax 

	return;
}


export fn __KeccakF1600(){
	 reg u256 c00;
	 reg u256 c14;
	 reg u256 d00;
	 reg u256 d14;
	 reg u256 state;
	 reg u256 t;

	// UNMATCHED: lea rhotates_left+96(%rip),rhotates_left

	// UNMATCHED: lea rhotates_right+96(%rip),rhotates_right

	// UNMATCHED: lea iotas(%rip),iotas

	// UNMATCHED: mov $24,%eax

	// UNMATCHED: jmp .Loop_avx2

	// UNMATCHED: .align 32

	// UNMATCHED: .Loop_avx2:

	//######################################## Theta
	c00 = #x86_VPSHUFD_256(state[2], (4u2)[1,0,3,2]);
	c14 = state[5] ^ state[3];
	t[2] = state[4] ^ state[6];
	c14 = c14 ^ state[1];
	c14 = c14 ^ t[2];
	t[4] = #x86_VPERMQ(c14, (4u2)[2,1,0,3]);
	c00 = c00 ^ state[2];
	t[0] = #x86_VPERMQ(c00, (4u2)[1,0,3,2]);
	t[1] = c14 >>4u64 63;
	t[2] = c14 +4u64 c14;
	t[1] = t[1] | t[2];
	d14 = #x86_VPERMQ(t[1], (4u2)[0,3,2,1]);
	d00 = t[1] ^ t[4];
	d00 = #x86_VPERMQ(d00, (4u2)[0,0,0,0]);
	c00 = c00 ^ state[0];
	c00 = c00 ^ t[0];
	t[0] = c00 >>4u64 63;
	t[1] = c00 +4u64 c00;
	t[1] = t[1] | t[0];
	state[2] = state[2] ^ d00;
	state[0] = state[0] ^ d00;
	d14 = #x86_VPBLENDD_256(d14, t[1], (4u2)[3,0,0,0]);
	t[4] = #x86_VPBLENDD_256(t[4], c00, (4u2)[0,0,0,3]);
	d14 = d14 ^ t[4];
	//######################################## Rho + Pi + pre-Chi shuffle
	t[3] = #x86_VPSLLV_4u64(state[2], B256(rhotates_left,0,96) );
	state[2] = #x86_VPSRLV_4u64(state[2], B256(rhotates_right,0,96) );
	state[2] = state[2] | t[3];
	state[3] = state[3] ^ d14;
	t[4] = #x86_VPSLLV_4u64(state[3], B256(rhotates_left,2,96) );
	state[3] = #x86_VPSRLV_4u64(state[3], B256(rhotates_right,2,96) );
	state[3] = state[3] | t[4];
	state[4] = state[4] ^ d14;
	t[5] = #x86_VPSLLV_4u64(state[4], B256(rhotates_left,3,96) );
	state[4] = #x86_VPSRLV_4u64(state[4], B256(rhotates_right,3,96) );
	state[4] = state[4] | t[5];
	state[5] = state[5] ^ d14;
	t[6] = #x86_VPSLLV_4u64(state[5], B256(rhotates_left,4,96) );
	state[5] = #x86_VPSRLV_4u64(state[5], B256(rhotates_right,4,96) );
	state[5] = state[5] | t[6];
	state[6] = state[6] ^ d14;
	t[3] = #x86_VPERMQ(state[2], (4u2)[2,0,3,1]);
	t[4] = #x86_VPERMQ(state[3], (4u2)[2,0,3,1]);
	t[7] = #x86_VPSLLV_4u64(state[6], B256(rhotates_left,5,96) );
	t[1] = #x86_VPSRLV_4u64(state[6], B256(rhotates_right,5,96) );
	t[1] = t[1] | t[7];
	state[1] = state[1] ^ d14;
	t[5] = #x86_VPERMQ(state[4], (4u2)[0,1,2,3]);
	t[6] = #x86_VPERMQ(state[5], (4u2)[1,3,0,2]);
	t[8] = #x86_VPSLLV_4u64(state[1], B256(rhotates_left,1,96) );
	t[2] = #x86_VPSRLV_4u64(state[1], B256(rhotates_right,1,96) );
	t[2] = t[2] | t[8];
	//######################################## Chi
	t[7] = #x86_VPSRLDQ_256(t[1], 8);
	t[0] = !t[1] & t[7];
	state[3] = #x86_VPBLENDD_256(t[2], t[6], (4u2)[0,0,3,0]);
	t[8] = #x86_VPBLENDD_256(t[4], t[2], (4u2)[0,0,3,0]);
	state[5] = #x86_VPBLENDD_256(t[3], t[4], (4u2)[0,0,3,0]);
	t[7] = #x86_VPBLENDD_256(t[2], t[3], (4u2)[0,0,3,0]);
	state[3] = #x86_VPBLENDD_256(state[3], t[4], (4u2)[0,3,0,0]);
	t[8] = #x86_VPBLENDD_256(t[8], t[5], (4u2)[0,3,0,0]);
	state[5] = #x86_VPBLENDD_256(state[5], t[2], (4u2)[0,3,0,0]);
	t[7] = #x86_VPBLENDD_256(t[7], t[6], (4u2)[0,3,0,0]);
	state[3] = #x86_VPBLENDD_256(state[3], t[5], (4u2)[3,0,0,0]);
	t[8] = #x86_VPBLENDD_256(t[8], t[6], (4u2)[3,0,0,0]);
	state[5] = #x86_VPBLENDD_256(state[5], t[6], (4u2)[3,0,0,0]);
	t[7] = #x86_VPBLENDD_256(t[7], t[4], (4u2)[3,0,0,0]);
	state[3] = !state[3] & t[8];
	state[5] = !state[5] & t[7];
	state[6] = #x86_VPBLENDD_256(t[5], t[2], (4u2)[0,0,3,0]);
	t[8] = #x86_VPBLENDD_256(t[3], t[5], (4u2)[0,0,3,0]);
	state[3] = state[3] ^ t[3];
	state[6] = #x86_VPBLENDD_256(state[6], t[3], (4u2)[0,3,0,0]);
	t[8] = #x86_VPBLENDD_256(t[8], t[4], (4u2)[0,3,0,0]);
	state[5] = state[5] ^ t[5];
	state[6] = #x86_VPBLENDD_256(state[6], t[4], (4u2)[3,0,0,0]);
	t[8] = #x86_VPBLENDD_256(t[8], t[2], (4u2)[3,0,0,0]);
	state[6] = !state[6] & t[8];
	state[6] = state[6] ^ t[6];
	state[4] = #x86_VPERMQ(t[1], (4u2)[0,1,3,2]);
	t[8] = #x86_VPBLENDD_256(state[4], state[0], (4u2)[0,3,0,0]);
	state[1] = #x86_VPERMQ(t[1], (4u2)[0,3,2,1]);
	state[1] = #x86_VPBLENDD_256(state[1], state[0], (4u2)[3,0,0,0]);
	state[1] = !state[1] & t[8];
	state[2] = #x86_VPBLENDD_256(t[4], t[5], (4u2)[0,0,3,0]);
	t[7] = #x86_VPBLENDD_256(t[6], t[4], (4u2)[0,0,3,0]);
	state[2] = #x86_VPBLENDD_256(state[2], t[6], (4u2)[0,3,0,0]);
	t[7] = #x86_VPBLENDD_256(t[7], t[3], (4u2)[0,3,0,0]);
	state[2] = #x86_VPBLENDD_256(state[2], t[3], (4u2)[3,0,0,0]);
	t[7] = #x86_VPBLENDD_256(t[7], t[5], (4u2)[3,0,0,0]);
	state[2] = !state[2] & t[7];
	state[2] = state[2] ^ t[2];
	t[0] = #x86_VPERMQ(t[0], (4u2)[0,0,0,0]);
	state[3] = #x86_VPERMQ(state[3], (4u2)[0,1,2,3]);
	state[5] = #x86_VPERMQ(state[5], (4u2)[2,0,3,1]);
	state[6] = #x86_VPERMQ(state[6], (4u2)[1,3,0,2]);
	state[4] = #x86_VPBLENDD_256(t[6], t[3], (4u2)[0,0,3,0]);
	t[7] = #x86_VPBLENDD_256(t[5], t[6], (4u2)[0,0,3,0]);
	state[4] = #x86_VPBLENDD_256(state[4], t[5], (4u2)[0,3,0,0]);
	t[7] = #x86_VPBLENDD_256(t[7], t[2], (4u2)[0,3,0,0]);
	state[4] = #x86_VPBLENDD_256(state[4], t[2], (4u2)[3,0,0,0]);
	t[7] = #x86_VPBLENDD_256(t[7], t[3], (4u2)[3,0,0,0]);
	state[4] = !state[4] & t[7];
	state[0] = state[0] ^ t[0];
	state[1] = state[1] ^ t[1];
	state[4] = state[4] ^ t[4];
	//######################################## Iota
	state[0] = state[0] ^ B256(iotas,0,0);
	// UNMATCHED: lea 32(iotas),iotas

	// UNMATCHED: dec %eax

	// UNMATCHED: jnz .Loop_avx2

	return;
}


